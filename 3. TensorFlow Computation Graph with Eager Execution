import tensorflow as tf
a = tf.constant([5.0, 3.0])
b = tf.constant([2.0, 7.0])
c = a + b
print("Eager Execution Output:", c.numpy())

@tf.function
def multiply_tensors(x, y):
    return x * y

result = multiply_tensors(a, b)
print("Graph Mode Output:", result.numpy())


THEORY -
### 1. TensorFlow Computation Graph

* TensorFlow works using a **computation graph** where nodes represent operations (e.g., addition, multiplication) and edges represent data (tensors).
* Earlier TensorFlow (v1.x) required users to **build graphs first, then run them** using `sess.run()`.
* TensorFlow (v2.x) introduced **Eager Execution** by default, making it more Pythonic and user-friendly.

---

### 2. Eager Execution

* Eager mode means operations are executed **immediately**, like standard Python code.
* Example:

  ```python
  a = tf.constant([5.0, 3.0])
  b = tf.constant([2.0, 7.0])
  c = a + b
  print(c.numpy())   # [7.0, 10.0]
  ```
* Here, the addition happens instantly, and you can view results without building a graph.
* Useful for debugging and experimentation.

---

### 3. Graph Mode with `@tf.function`

* `@tf.function` converts a Python function into a **TensorFlow computation graph**.
* Instead of executing line by line, TensorFlow builds a graph of operations and runs it more efficiently.
* Example:

  ```python
  @tf.function
  def multiply_tensors(x, y):
      return x * y
  ```
* TensorFlow compiles this into a graph internally.
* Running it is faster than eager execution (especially on GPUs/TPUs).

---

### 4. Code Output Explanation

1. **Eager Execution Output:**

   ```
   [7. 10.]
   ```

   (since \[5+2, 3+7] = \[7, 10])

2. **Graph Mode Output:**

   ```
   [10. 21.]
   ```

   (since \[5*2, 3*7] = \[10, 21])

---

### 5. Why Both Modes Matter in Generative AI

* **Eager Execution:** Easy debugging, rapid prototyping of neural networks.
* **Graph Mode:** Optimized execution, required for training **large generative models (GANs, Transformers, Diffusion Models)** efficiently.

