from gensim.models import Word2Vec

sentences = [
    ["artificial", "intelligence", "is", "cool"],
    ["machine", "learning", "is", "fun"],
    ["ai", "learning", "uses", "neural", "networks"]
]
model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, sg=1)

print("Vector for 'learning':", model.wv['learning'])
print("Most similar to 'learning':", model.wv.most_similar('learning'))

THEORY -

### 1. Word Embeddings in Generative AI

* Words are converted into **dense vectors** that capture semantic meaning.
* Unlike one-hot encoding (sparse, no semantic info), embeddings place similar words closer in vector space.
* Used in NLP tasks like text generation, machine translation, and chatbots.

---

### 2. Word2Vec Model

* Developed by Google (Mikolov et al., 2013).
* Learns embeddings by predicting context words from a target word (CBOW) or predicting a target word from context (Skip-Gram).
* Parameters in your code:

  * `vector_size=10`: Each word is represented as a 10-dimensional vector.
  * `window=2`: Context window size = 2 (looks at 2 words before and after).
  * `min_count=1`: Ignores words that appear less than 1 time (keeps all words here).
  * `sg=1`: Skip-Gram model (better for small data, rare words).

---

### 3. How Training Works

* Example sentence: `"machine learning is fun"`.
* Skip-Gram tries to predict context words given `"learning"`.
* Embeddings are adjusted so that words appearing in similar contexts (like “ai” and “learning”) end up closer in vector space.

---

### 4. Code Execution

1. **Get Vector Representation**

   ```python
   print("Vector for 'learning':", model.wv['learning'])
   ```

   * Returns a 10-dimensional vector, e.g. `[0.012, -0.034, ...]`.
   * Each dimension captures latent semantic information.

2. **Find Most Similar Words**

   ```python
   print("Most similar to 'learning':", model.wv.most_similar('learning'))
   ```

   * Returns a list of words most similar to `"learning"` based on **cosine similarity** between vectors.
   * Example output: `[('ai', 0.85), ('machine', 0.77), ...]`.

---

### 5. Why Word2Vec is Useful in Generative AI

* Embeddings allow models to generate **meaningful, context-aware text**.
* Instead of treating words as separate symbols, it captures **semantic relationships**:

  * `"king - man + woman ≈ queen"`
  * `"ai ≈ artificial intelligence"`

